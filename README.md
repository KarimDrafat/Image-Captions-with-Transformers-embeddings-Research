# Image-Captions-with-Transformers-embeddings-Research

## Abstract 

Image captioning (IC) refers to the process of generating descrip- tive textual representations of visual content. Transformer-based mod- els represent the state-of-the-art in various NLP tasks, including IC (Ondeng et al.,2023). They have revolutionized IC by leveraging self- attention mechanisms to efficiently capture contextual information in parallel, surpassing traditional CNN-LSTM systems that process information sequentially. However, despite recent advancements in deep learning and NLP, IC encounters challenges in handling diverse visual and linguistic contexts effectively.  These challenges include accurately interpreting complex scenes, understanding nuanced lin- guistic expressions, and generating contextually coherent captions across  a  wide  range  of  images.   This  research  introduced  a  novel architecture for IC. The goal is to combine the traditional encoder- decoder (CNN+LSTM) models with transformer-based models such as BERT or GPT. Instead of using only a pre-trained CNN for image feature extraction, the structure incorporates BERT as a second en- coder for captions. The decoder part remained LSTM, resulting in a multimodal/hybrid architecture.  Additionally, the study intended to evaluate and compare the use of GPT in place of BERT within the same architecture, and experiment with fine-tuning either the entire architecture or just the LSTM component. As a baseline reference to compare with our research approach, the study utilized a soft atten- tion model integrated between the CNN and LSTM layers similar to Xu et al. (2015) approach. Evaluation on the Flickr30k dataset showed promising results in improving caption accuracy and semantic rich- ness, highlighting advancements in multimodal image understanding and description.  The models, particularly those utilizing GPT em- beddings with fine-tuning, demonstrated competitive performance compared to recent architectures (Alqahtani et al.,2024; Patel & Varier, 2020; Verma et al.,2024), and outperformed the baseline model. More- over, we evaluated the best and least scored predicted captions using BLEU, METEOR, and CIDEr scores. The analysis revealed that com- mon errors included repetition, missing information, incorrect details, and  grammar  issues.   A  detailed  evaluation  of  the  lowest  scoring captions revealed that errors in missing information and incorrect details were more prevalent, while repetition and grammar issues were less frequent but still notable. These findings highlight areas for improvement in IC models
